{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Filtragem das Letras***\n",
    "---\n",
    "1. Coleta do *dataset* [4Mula](https://github.com/4mulaDataset/4mula) e do arquivo de [*stopwords*](https://gist.github.com/alopes/5358189)\n",
    "2. Filtragem dos campos da tabela irrelevantes para a aplicação\n",
    "3. Formatação das letras\n",
    "4. Utilização da bibliotéca [NLTK](https://www.nltk.org/) para *tokenização* das palavras\n",
    "5. Filtragem das stopwords\n",
    "6. Utilização da bibliotéca [Spacy](https://spacy.io/) para a lematização dos termos\n",
    "7. Anexação das letras filtradas à coluna `filtered lyrics`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = \"../files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = open(path + \"stopwords.txt\" ,\"r\")\n",
    "_stopwords = f.read()\n",
    "f.close()\n",
    "stopwords = set()\n",
    "for word in _stopwords.split(\"\\n\"):\n",
    "    stopwords.add(word.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet( path + \"4mula_metadata.parquet\" )\n",
    "m = df[df['music_lang'] == 'pt-br']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = m.drop(columns=['music_id', 'art_id','music_lang','art_rank','related_art',\n",
    "                    'related_music','main_genre', 'related_genre'])\n",
    "m = m.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in m.iterrows():\n",
    "    row['music_lyrics'] = row['music_lyrics'].replace('\\\\n',' ')\n",
    "    row['music_lyrics'] = row['music_lyrics'].replace('  ',' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lyrics = []\n",
    "for i, row in m.iterrows():\n",
    "    doc = row['music_lyrics'].translate(str.maketrans('','',string.punctuation))\n",
    "    words = word_tokenize(doc.lower())\n",
    "    filtered = [word for word in words if not word in stopwords]            \n",
    "    filtered_lyrics.append(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"pt_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_lyrics = []\n",
    "for i, row in m.iterrows():\n",
    "    doc = ' '.join(filtered_lyrics[i])\n",
    "    filtered=\"\"\n",
    "    for token in nlp(doc):\n",
    "        filtered += token.lemma_ + ' '\n",
    "    lemmatized_lyrics.append(filtered[:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m['filtered_lyrics'] = lemmatized_lyrics\n",
    "m = m[['art_name','music_name','music_lyrics','filtered_lyrics']]\n",
    "m.to_csv(path + '4mula_filtered.csv',index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
